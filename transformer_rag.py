#!/usr/bin/env python3
"""
íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ - ë¬´ë£Œ ë¡œì»¬ LLM ëª¨ë¸ ì‚¬ìš©
"""

import os
import logging
import re
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    pipeline,
    BitsAndBytesConfig
)
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class TransformerTourismRAG:
    def __init__(self, data_folder="./data", similarity_threshold=0.1):
        self.data_folder = data_folder
        self.docs = []
        self.metainfo = []
        
        # í•œêµ­ì–´ ìµœì í™”ëœ TF-IDF ë²¡í„°ë¼ì´ì €
        self.vectorizer = TfidfVectorizer(
            max_features=1500,
            ngram_range=(1, 2),
            lowercase=True,
            min_df=1,
            max_df=0.95,
            token_pattern=r'(?u)\b\w\w+\b|[ê°€-í£]+',
            stop_words=None
        )
        
        self.doc_vectors = None
        self.similarity_threshold = similarity_threshold
        self.is_initialized = False
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê´€ë ¨
        self.model_name = None
        self.tokenizer = None
        self.model = None
        self.generator = None
        
    def setup_model(self):
        """ë¬´ë£Œ í•œêµ­ì–´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ì„¤ì •"""
        try:
            logger.info("íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # í•œêµ­ì–´ ì§€ì› ë¬´ë£Œ ëª¨ë¸ë“¤ (í¬ê¸° ìˆœ, ê²½ëŸ‰í™”)
            model_options = [
                "microsoft/DialoGPT-small",   # ë§¤ìš° ê²½ëŸ‰, ëŒ€í™”í˜•
                "distilgpt2",                 # ê²½ëŸ‰ GPT-2
                "gpt2",                       # ê¸°ë³¸ GPT-2
            ]
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            # ì²« ë²ˆì§¸ë¡œ ì„±ê³µí•˜ëŠ” ëª¨ë¸ ì‚¬ìš©
            for model_name in model_options:
                try:
                    logger.info(f"ëª¨ë¸ ì‹œë„: {model_name}")
                    
                    if "gpt" in model_name.lower() or "dialogpt" in model_name.lower():
                        # ìƒì„± ëª¨ë¸
                        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                        
                        # íŒ¨ë”© í† í° ì„¤ì •
                        if self.tokenizer.pad_token is None:
                            self.tokenizer.pad_token = self.tokenizer.eos_token
                        
                        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
                        if device == "cuda":
                            quantization_config = BitsAndBytesConfig(
                                load_in_4bit=True,
                                bnb_4bit_compute_dtype=torch.float16
                            )
                            self.model = AutoModelForCausalLM.from_pretrained(
                                model_name,
                                quantization_config=quantization_config,
                                device_map="auto",
                                torch_dtype=torch.float16
                            )
                        else:
                            self.model = AutoModelForCausalLM.from_pretrained(
                                model_name,
                                torch_dtype=torch.float32
                            )
                            self.model.to(device)
                        
                        # íŒŒì´í”„ë¼ì¸ ìƒì„±
                        self.generator = pipeline(
                            "text-generation",
                            model=self.model,
                            tokenizer=self.tokenizer,
                            device=0 if device == "cuda" else -1,
                            do_sample=True,
                            temperature=0.7,
                            max_length=512,
                            pad_token_id=self.tokenizer.pad_token_id
                        )
                        
                        self.model_name = model_name
                        logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        return True
                        
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
            
            # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ì‹œ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í´ë°±
            logger.warning("ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ í´ë°± ì‚¬ìš©")
            self.generator = None  # ê·œì¹™ ê¸°ë°˜ ì‚¬ìš©
            self.model_name = "rule-based-fallback"
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def clean_korean_text(self, text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        
        # 2. ë„ì–´ì“°ê¸° ë¬¸ì œ í•´ê²° (í•œêµ­ì–´ íŠ¹í™”)
        text = re.sub(r'([ê°€-í£])\s+([ê°€-í£])(?=\s|[ê°€-í£]|$)', r'\1\2', text)
        
        # 3. ìˆ«ìì™€ ë‹¨ìœ„ ê²°í•©
        text = re.sub(r'(\d+)\s*([%ëª…ê°œê±´ì²œë§Œì–µì›ë‹¬ëŸ¬])', r'\1\2', text)
        
        # 4. ì˜ì–´ ë‹¨ì–´ ê²°í•©
        text = re.sub(r'([A-Za-z])\s+([A-Za-z])\s+([A-Za-z])', r'\1\2\3', text)
        
        # 5. ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?%-]', ' ', text)
        
        return text.strip()

    def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("íŠ¸ëœìŠ¤í¬ë¨¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ëª¨ë¸ ì„¤ì •
            if not self.setup_model():
                raise Exception("íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            
            # 2. ë¬¸ì„œ ë¡œë“œ
            self.load_pdfs()
            
            # 3. ë²¡í„° DB êµ¬ì¶•
            if self.docs:
                self.build_vectors()
                self.is_initialized = True
                logger.info("íŠ¸ëœìŠ¤í¬ë¨¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            else:
                logger.warning("ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def load_pdfs(self):
        """PDF ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        logger.info("PDF íŒŒì¼ ë¡œë“œ ì‹œì‘...")
        
        if not os.path.exists(self.data_folder):
            return
            
        pdf_files = [f for f in os.listdir(self.data_folder) if f.endswith('.pdf')]
        
        for pdf in tqdm(pdf_files, desc="PDF ì²˜ë¦¬"):
            try:
                path = os.path.join(self.data_folder, pdf)
                reader = PdfReader(path)
                
                for i, page in enumerate(reader.pages):
                    raw_text = page.extract_text()
                    if raw_text and len(raw_text.strip()) > 50:
                        
                        # í•œêµ­ì–´ ìµœì í™” ì „ì²˜ë¦¬
                        cleaned = self.clean_korean_text(raw_text)
                        
                        if len(cleaned) > 100:
                            # ì ë‹¹í•œ í¬ê¸°ë¡œ ë¶„í• 
                            if len(cleaned) > 1000:
                                chunks = self.split_text(cleaned, 800)
                                for j, chunk in enumerate(chunks):
                                    self.docs.append(chunk)
                                    self.metainfo.append(f"{pdf} - page {i+1} - part {j+1}")
                            else:
                                self.docs.append(cleaned)
                                self.metainfo.append(f"{pdf} - page {i+1}")
                                
            except Exception as e:
                logger.error(f"PDF {pdf} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
        logger.info(f"ì´ {len(self.docs)}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")

    def split_text(self, text, max_length):
        """í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• """
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = re.split(r'[.!?]\s*', text)
        
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks

    def build_vectors(self):
        """ë²¡í„° DB êµ¬ì¶•"""
        logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        if not self.docs:
            return
        
        try:
            self.doc_vectors = self.vectorizer.fit_transform(self.docs)
            logger.info(f"ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {self.doc_vectors.shape}")
            
        except Exception as e:
            logger.error(f"ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            raise

    def search(self, question, top_k=3):
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.is_initialized or self.doc_vectors is None:
            return [], []
        
        try:
            # ì§ˆë¬¸ ì „ì²˜ë¦¬
            cleaned_question = self.clean_korean_text(question)
            
            # ë²¡í„°í™”
            query_vector = self.vectorizer.transform([cleaned_question])
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            top_indices = similarities.argsort()[-top_k*2:][::-1]
            
            retrieved = []
            sources = []
            
            for idx in top_indices:
                if similarities[idx] >= self.similarity_threshold and len(retrieved) < top_k:
                    retrieved.append(self.docs[idx])
                    score = round(similarities[idx], 3)
                    sources.append(f"{self.metainfo[idx]} (ìœ ì‚¬ë„ {score})")
            
            return retrieved, sources
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], []

    def generate_llm_answer(self, question, context_docs):
        """íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±"""
        if not self.generator:
            # ê·œì¹™ ê¸°ë°˜ í´ë°±
            return self.generate_rule_based_answer(question, context_docs)
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context = "\n".join(context_docs[:2])  # ìƒìœ„ 2ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
            context = context[:1000]  # ê¸¸ì´ ì œí•œ
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í•œêµ­ì–´)
            prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            # ëª¨ë¸ ì¶”ë¡ 
            if "gpt" in self.model_name.lower() or "dialogpt" in self.model_name.lower():
                # GPT ê³„ì—´ ëª¨ë¸
                responses = self.generator(
                    prompt,
                    max_new_tokens=200,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.pad_token_id,
                    num_return_sequences=1
                )
                
                generated_text = responses[0]['generated_text']
                
                # í”„ë¡¬í”„íŠ¸ ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if "ë‹µë³€:" in generated_text:
                    answer = generated_text.split("ë‹µë³€:")[-1].strip()
                else:
                    answer = generated_text[len(prompt):].strip()
                    
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ (BART, BERT ë“±)
                answer = "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í†µí•´ ì²˜ë¦¬ëœ ë‹µë³€ì…ë‹ˆë‹¤."
            
            # ë‹µë³€ í›„ì²˜ë¦¬
            answer = self.post_process_answer(answer, question)
            
            return answer
            
        except Exception as e:
            logger.error(f"LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def post_process_answer(self, answer, question):
        """ë‹µë³€ í›„ì²˜ë¦¬"""
        if not answer or len(answer.strip()) < 10:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°
        lines = answer.split('\n')
        unique_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in unique_lines and len(line) > 5:
                unique_lines.append(line)
        
        # ë„ˆë¬´ ê¸´ ë‹µë³€ ìë¥´ê¸°
        processed = ' '.join(unique_lines)
        if len(processed) > 500:
            processed = processed[:500] + "..."
        
        return processed

    def generate_rule_based_answer(self, question, context_docs):
        """íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        if not context_docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        key_sentences = []
        for doc in context_docs[:2]:
            sentences = [s.strip() for s in re.split(r'[.!?]', doc) if s.strip()]
            for sentence in sentences:
                if (len(sentence) > 20 and len(sentence) < 200 and
                    any(word in sentence for word in ['íŠ¹ì„±', 'ë¹„ìœ¨', 'ë†’ìŒ', 'ë‚®ìŒ', 'ì¦ê°€', 'ê°ì†Œ', 'ì„ í˜¸', 'ê´€ê´‘', 'ì—¬í–‰'])):
                    key_sentences.append(sentence)
                if len(key_sentences) >= 3:
                    break
            if len(key_sentences) >= 3:
                break
        
        if key_sentences:
            answer = f"ì§ˆë¬¸: {question}\n\n"
            answer += "ê´€ë ¨ ì •ë³´:\n"
            for i, sentence in enumerate(key_sentences[:3], 1):
                answer += f"{i}. {sentence}.\n"
            answer += "\nğŸ’¡ ë” ìì„¸í•œ ì •ë³´ëŠ” ì›ë³¸ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
            return answer
        else:
            # ì»¨í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
            context_text = " ".join(context_docs[:1])[:300]
            return f"ì§ˆë¬¸: {question}\n\nê´€ë ¨ ë‚´ìš©:\n{context_text}..."

    def generate_answer(self, question):
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        if not self.is_initialized:
            return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs, sources = self.search(question, top_k=3)
            
            if not retrieved_docs:
                return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.", []
            
            # 2. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
            llm_answer = self.generate_llm_answer(question, retrieved_docs)
            
            return llm_answer, sources
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", []

    def health_check(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ"""
        return {
            "initialized": self.is_initialized,
            "documents_loaded": len(self.docs),
            "model_loaded": self.generator is not None,
            "model_name": self.model_name,
            "vector_db_ready": self.doc_vectors is not None,
            "version": "transformer_based",
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        }