#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ RAG ì‹œìŠ¤í…œ - í•œêµ­ì–´ ì²˜ë¦¬ ê°œì„ 
"""

import os
import logging
import re
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from tqdm import tqdm

logger = logging.getLogger(__name__)

class FixedTourismRAG:
    def __init__(self, data_folder="./data", similarity_threshold=0.1):
        self.data_folder = data_folder
        self.docs = []
        self.metainfo = []
        
        # í•œêµ­ì–´ ìµœì í™”ëœ TF-IDF ë²¡í„°ë¼ì´ì €
        self.vectorizer = TfidfVectorizer(
            max_features=1500,
            ngram_range=(1, 2),  # 1-gram, 2-gram
            lowercase=True,
            min_df=1,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„ë¥¼ 1ë¡œ ì„¤ì •
            max_df=0.95,
            token_pattern=r'(?u)\b\w\w+\b|[ê°€-í£]+',  # í•œêµ­ì–´ + ì˜ì–´ í† í°
            stop_words=None  # í•œêµ­ì–´ì—ëŠ” ë¶ˆìš©ì–´ ì œê±° ì•ˆí•¨
        )
        
        self.doc_vectors = None
        self.similarity_threshold = similarity_threshold
        self.is_initialized = False

    def clean_korean_text(self, text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        
        # 2. ë„ì–´ì“°ê¸° ë¬¸ì œ í•´ê²° (í•œêµ­ì–´ íŠ¹í™”)
        # "í•œ ë¥˜" -> "í•œë¥˜", "ê´€ ê´‘ ê°" -> "ê´€ê´‘ê°"
        text = re.sub(r'([ê°€-í£])\s+([ê°€-í£])(?=\s|[ê°€-í£]|$)', r'\1\2', text)
        
        # 3. ìˆ«ìì™€ ë‹¨ìœ„ ê²°í•©
        text = re.sub(r'(\d+)\s*([%ëª…ê°œê±´ì²œë§Œì–µì›ë‹¬ëŸ¬])', r'\1\2', text)
        
        # 4. ì˜ì–´ ë‹¨ì–´ ê²°í•©
        text = re.sub(r'([A-Za-z])\s+([A-Za-z])\s+([A-Za-z])', r'\1\2\3', text)
        
        # 5. ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?%-]', ' ', text)
        
        return text.strip()

    def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("ìˆ˜ì •ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            self.load_pdfs()
            
            if self.docs:
                self.build_vectors()
                self.is_initialized = True
                logger.info("ìˆ˜ì •ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            else:
                logger.warning("ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def load_pdfs(self):
        """PDF ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        logger.info("PDF íŒŒì¼ ë¡œë“œ ì‹œì‘...")
        
        if not os.path.exists(self.data_folder):
            return
            
        pdf_files = [f for f in os.listdir(self.data_folder) if f.endswith('.pdf')]
        
        for pdf in tqdm(pdf_files, desc="PDF ì²˜ë¦¬"):
            try:
                path = os.path.join(self.data_folder, pdf)
                reader = PdfReader(path)
                
                for i, page in enumerate(reader.pages):
                    raw_text = page.extract_text()
                    if raw_text and len(raw_text.strip()) > 50:
                        
                        # í•œêµ­ì–´ ìµœì í™” ì „ì²˜ë¦¬
                        cleaned = self.clean_korean_text(raw_text)
                        
                        if len(cleaned) > 100:
                            # ì ë‹¹í•œ í¬ê¸°ë¡œ ë¶„í• 
                            if len(cleaned) > 1000:
                                chunks = self.split_text(cleaned, 800)
                                for j, chunk in enumerate(chunks):
                                    self.docs.append(chunk)
                                    self.metainfo.append(f"{pdf} - page {i+1} - part {j+1}")
                            else:
                                self.docs.append(cleaned)
                                self.metainfo.append(f"{pdf} - page {i+1}")
                                
            except Exception as e:
                logger.error(f"PDF {pdf} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                
        logger.info(f"ì´ {len(self.docs)}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")

    def split_text(self, text, max_length):
        """í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• """
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = re.split(r'[.!?]\s*', text)
        
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks

    def build_vectors(self):
        """ë²¡í„° DB êµ¬ì¶•"""
        logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        if not self.docs:
            return
        
        try:
            self.doc_vectors = self.vectorizer.fit_transform(self.docs)
            logger.info(f"ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {self.doc_vectors.shape}")
            
        except Exception as e:
            logger.error(f"ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            raise

    def search(self, question, top_k=5):
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.is_initialized or self.doc_vectors is None:
            return [], []
        
        try:
            # ì§ˆë¬¸ ì „ì²˜ë¦¬
            cleaned_question = self.clean_korean_text(question)
            
            # ë²¡í„°í™”
            query_vector = self.vectorizer.transform([cleaned_question])
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            top_indices = similarities.argsort()[-top_k*2:][::-1]
            
            retrieved = []
            sources = []
            
            for idx in top_indices:
                if similarities[idx] >= self.similarity_threshold and len(retrieved) < top_k:
                    retrieved.append(self.docs[idx])
                    score = round(similarities[idx], 3)
                    sources.append(f"{self.metainfo[idx]} (ìœ ì‚¬ë„ {score})")
            
            return retrieved, sources
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], []

    def generate_structured_answer(self, question, retrieved_docs, sources):
        """êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
        if not retrieved_docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.", []
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['íŠ¹ì„±', 'íŠ¹ì§•']):
            title = "ğŸ“Š **ì£¼ìš” íŠ¹ì„±**"
        elif any(word in question_lower for word in ['ì†Œë¹„', 'êµ¬ë§¤', 'ì‡¼í•‘']):
            title = "ğŸ’° **ì†Œë¹„ íŒ¨í„´**"
        elif any(word in question_lower for word in ['ë°©ë¬¸', 'ì—¬í–‰', 'í™œë™']):
            title = "ğŸ¯ **ë°©ë¬¸ í–‰ë™**"
        else:
            title = "ğŸ“‹ **ê´€ë ¨ ì •ë³´**"
        
        # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        key_points = []
        
        for doc in retrieved_docs[:3]:
            # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë“¤ ì¶”ì¶œ
            sentences = [s.strip() for s in re.split(r'[.!?]', doc) if s.strip()]
            
            for sentence in sentences:
                if (len(sentence) > 30 and len(sentence) < 150 and
                    any(word in sentence for word in ['íŠ¹ì„±', 'ë¹„ìœ¨', 'ë†’ìŒ', 'ë‚®ìŒ', 'ì¦ê°€', 'ê°ì†Œ', 'ì„ í˜¸']) and
                    not sentence.startswith('í‘œ') and
                    not sentence.startswith('ê·¸ë¦¼')):
                    
                    key_points.append(sentence)
                    
                if len(key_points) >= 4:
                    break
                    
            if len(key_points) >= 4:
                break
        
        # ë‹µë³€ êµ¬ì„±
        if key_points:
            answer_parts = [title, ""]
            
            for i, point in enumerate(key_points[:4], 1):
                # ë¬¸ì¥ ì •ë¦¬
                clean_point = point
                if not clean_point.endswith('.'):
                    clean_point += "."
                
                answer_parts.append(f"**{i}.** {clean_point}")
            
            answer_parts.append("")
            answer_parts.append("ğŸ’¡ *ë” ìì„¸í•œ ë‚´ìš©ì€ ì¶œì²˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.*")
            
            return "\n".join(answer_parts), sources[:3]
        else:
            # í´ë°±: ì›ë¬¸ì˜ ì¼ë¶€ë¥¼ ì •ë¦¬í•´ì„œ ë°˜í™˜
            context = " ".join(retrieved_docs[:2])
            return f"{title}\n\n{context[:300]}...", sources[:3]

    def generate_answer(self, question):
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        if not self.is_initialized:
            return "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        try:
            retrieved_docs, sources = self.search(question, top_k=5)
            return self.generate_structured_answer(question, retrieved_docs, sources)
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", []

    def health_check(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ"""
        return {
            "initialized": self.is_initialized,
            "documents_loaded": len(self.docs),
            "model_loaded": self.doc_vectors is not None,
            "vector_db_ready": self.doc_vectors is not None,
            "version": "fixed_korean"
        }