#!/usr/bin/env python3
"""
ê°œì„ ëœ RAG ì‹œìŠ¤í…œ - ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±
"""

import os
import logging
import re
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from tqdm import tqdm
import requests
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedTourismRAG:
    def __init__(self, data_folder="./data", similarity_threshold=0.1):
        self.data_folder = data_folder
        self.docs = []
        self.metainfo = []
        self.vectorizer = TfidfVectorizer(
            max_features=2000,  # íŠ¹ì§• ìˆ˜ ì¦ê°€
            stop_words='english',
            ngram_range=(1, 3),  # 3-gramê¹Œì§€ í™•ì¥
            lowercase=True,
            min_df=1,
            max_df=0.95
        )
        self.doc_vectors = None
        self.similarity_threshold = similarity_threshold
        self.is_initialized = False
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì‚¬ì „
        self.keywords_dict = {
            'í•œë¥˜': ['K-pop', 'ë“œë¼ë§ˆ', 'ì˜í™”', 'í•œë¥˜ì½˜í…ì¸ ', 'BTS', 'ë¸”ë™í•‘í¬'],
            'ê´€ê´‘': ['ë°©ë¬¸', 'ì—¬í–‰', 'ê´€ê´‘ê°', 'ì™¸êµ­ì¸', 'ê´€ê´‘ì§€', 'ì—¬í–‰ê°'],
            'íŠ¹ì„±': ['íŠ¹ì§•', 'ì„±í–¥', 'í–‰ë™', 'íŒ¨í„´', 'ê²½í–¥'],
            'ì†Œë¹„': ['êµ¬ë§¤', 'ì§€ì¶œ', 'ì†Œë¹„íŒ¨í„´', 'ì†Œë¹„ì„±í–¥'],
            'ë¬¸í™”': ['ì „í†µë¬¸í™”', 'ë¬¸í™”ì²´í—˜', 'ë¬¸í™”ì½˜í…ì¸ '],
        }

    def initialize(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("ê°œì„ ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            self.load_pdfs()
            if self.docs:
                self.build_tfidf_vectors()
                self.is_initialized = True
                logger.info("ê°œì„ ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            else:
                logger.warning("PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def clean_text(self, text):
        """í–¥ìƒëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ë° ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'[^\w\sê°€-í£.,!?()%-]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'\b\d{4}\b', ' ', text)  # ì—°ë„ ì œê±°
        text = re.sub(r'\b[A-Z]{2,}\b', lambda m: m.group().lower(), text)  # ëŒ€ë¬¸ì ì •ë¦¬
        return text.strip()

    def load_pdfs(self):
        """PDF íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
        logger.info("PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘")
        
        if not os.path.exists(self.data_folder):
            os.makedirs(self.data_folder)
            return
            
        pdf_files = [f for f in os.listdir(self.data_folder) if f.endswith('.pdf')]
        
        for pdf in tqdm(pdf_files, desc="PDF ì²˜ë¦¬ ì¤‘"):
            try:
                path = os.path.join(self.data_folder, pdf)
                reader = PdfReader(path)
                
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text and len(text.strip()) > 50:
                        # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• í•˜ì—¬ ì €ì¥
                        paragraphs = text.split('\n\n')
                        for j, paragraph in enumerate(paragraphs):
                            cleaned_text = self.clean_text(paragraph)
                            if len(cleaned_text) > 100:  # ìµœì†Œ ê¸¸ì´ í™•ë³´
                                self.docs.append(cleaned_text)
                                self.metainfo.append(f"{pdf} - page {i+1}, para {j+1}")
                                
            except Exception as e:
                logger.error(f"PDF íŒŒì¼ {pdf} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
        logger.info(f"ì´ {len(self.docs)}ê°œì˜ ë¬¸ë‹¨ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

    def build_tfidf_vectors(self):
        """TF-IDF ë²¡í„° ìƒì„±"""
        logger.info("TF-IDF ë²¡í„° ìƒì„± ì¤‘...")
        if self.docs:
            self.doc_vectors = self.vectorizer.fit_transform(self.docs)
            logger.info("TF-IDF ë²¡í„° ìƒì„± ì™„ë£Œ")

    def query(self, question, top_k=5):
        """í–¥ìƒëœ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.is_initialized or self.doc_vectors is None:
            return [], []
            
        # ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ í™•ì¥
        expanded_question = self.expand_question(question)
        query_vector = self.vectorizer.transform([expanded_question])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
        
        # ìƒìœ„ kê°œ ë¬¸ì„œ ì„ íƒ
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        retrieved = []
        sources = []
        
        for idx in top_indices:
            if similarities[idx] >= self.similarity_threshold:
                retrieved.append(self.docs[idx])
                sim_score = round(similarities[idx], 3)
                sources.append(f"{self.metainfo[idx]} (ìœ ì‚¬ë„ {sim_score})")
        
        return retrieved, sources

    def expand_question(self, question):
        """ì§ˆë¬¸ í‚¤ì›Œë“œ í™•ì¥"""
        expanded = question
        
        # í‚¤ì›Œë“œ ì‚¬ì „ì„ í™œìš©í•œ í™•ì¥
        for key, synonyms in self.keywords_dict.items():
            if key in question:
                expanded += " " + " ".join(synonyms)
        
        return expanded

    def extract_key_information(self, text, question):
        """í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        
        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ë“¤ ì¶”ì¶œ
        relevant_sentences = []
        question_words = set(re.findall(r'\b\w+\b', question.lower()))
        
        for sentence in sentences:
            sentence_words = set(re.findall(r'\b\w+\b', sentence.lower()))
            if len(question_words & sentence_words) >= 2 and len(sentence.strip()) > 20:
                relevant_sentences.append(sentence.strip())
        
        return relevant_sentences[:3]  # ìƒìœ„ 3ê°œ ë¬¸ì¥ë§Œ

    def structure_answer(self, question, retrieved_docs, sources):
        """êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        key_info = []
        for doc in retrieved_docs[:3]:
            extracted = self.extract_key_information(doc, question)
            key_info.extend(extracted)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_info = list(dict.fromkeys(key_info))[:5]
        
        # ë‹µë³€ êµ¬ì¡°í™”
        if unique_info:
            answer_parts = []
            
            # ì£¼ìš” ë‚´ìš© ìš”ì•½
            if 'íŠ¹ì„±' in question or 'íŠ¹ì§•' in question:
                answer_parts.append("ğŸ“Š ì£¼ìš” íŠ¹ì„±:")
                for i, info in enumerate(unique_info[:3], 1):
                    answer_parts.append(f"  {i}. {info}")
            
            elif 'í•œë¥˜' in question:
                answer_parts.append("ğŸ­ í•œë¥˜ ê´€ë ¨ ì •ë³´:")
                for i, info in enumerate(unique_info[:3], 1):
                    answer_parts.append(f"  â€¢ {info}")
            
            else:
                answer_parts.append("ğŸ’¡ ê´€ë ¨ ì •ë³´:")
                for i, info in enumerate(unique_info[:3], 1):
                    answer_parts.append(f"  - {info}")
            
            if len(unique_info) > 3:
                answer_parts.append("\nğŸ“‹ ì¶”ê°€ ì •ë³´ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            final_answer = "\n".join(answer_parts)
        else:
            # í´ë°±: ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë§Œ ì •ë¦¬í•´ì„œ ë°˜í™˜
            context = " ".join(retrieved_docs[:2])
            sentences = re.split(r'[.!?]', context)[:3]
            final_answer = "ê´€ë ¨ ì •ë³´:\n" + "\n".join([f"â€¢ {s.strip()}" for s in sentences if s.strip()])
        
        return final_answer, sources[:3]

    def generate_answer(self, question):
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        if not self.is_initialized:
            return "ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
            
        try:
            retrieved_docs, sources = self.query(question, top_k=8)
            return self.structure_answer(question, retrieved_docs, sources)
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", []

    def health_check(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        return {
            "initialized": self.is_initialized,
            "documents_loaded": len(self.docs),
            "model_loaded": self.doc_vectors is not None,
            "vector_db_ready": self.doc_vectors is not None
        }